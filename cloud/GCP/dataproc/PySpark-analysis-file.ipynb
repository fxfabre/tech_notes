{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "## Migrating from Spark to BigQuery via Dataproc -- Part 1\n\n* [Part 1](01_spark.ipynb): The original Spark code, now running on Dataproc (lift-and-shift).\n* [Part 2](02_gcs.ipynb): Replace HDFS by Google Cloud Storage. This enables job-specific-clusters. (cloud-native)\n* [Part 3](03_automate.ipynb): Automate everything, so that we can run in a job-specific cluster. (cloud-optimized)\n* [Part 4](04_bigquery.ipynb): Load CSV into BigQuery, use BigQuery. (modernize)\n* [Part 5](05_functions.ipynb): Using Cloud Functions, launch analysis every time there is a new file in the bucket. (serverless)\n"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Writing spark_analysis.py\n"}], "source": "%%writefile spark_analysis.py\nimport matplotlib\nmatplotlib.use('agg')\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--bucket\", help=\"bucket for input and output\")\nargs = parser.parse_args()\nBUCKET = args.bucket"}, {"cell_type": "markdown", "metadata": {}, "source": "### Reading in data\n\nThe data are CSV files. In Spark, these can be read using textFile and splitting rows on commas."}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\n\nfrom pyspark.sql import SparkSession, SQLContext, Row\n\ngcs_bucket='qwiklabs-gcp-01-85289ef223f9'\nspark = SparkSession.builder.appName(\"kdd\").getOrCreate()\nsc = spark.sparkContext\ndata_file = \"gs://\" + gcs_bucket + \"//kddcup.data_10_percent.gz\"\nraw_rdd = sc.textFile(data_file).cache()\nraw_rdd.take(5)"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\n\ncsv_rdd = raw_rdd.map(lambda row: row.split(\",\"))\nparsed_rdd = csv_rdd.map(lambda r: Row(\n    duration=int(r[0]), \n    protocol_type=r[1],\n    service=r[2],\n    flag=r[3],\n    src_bytes=int(r[4]),\n    dst_bytes=int(r[5]),\n    wrong_fragment=int(r[7]),\n    urgent=int(r[8]),\n    hot=int(r[9]),\n    num_failed_logins=int(r[10]),\n    num_compromised=int(r[12]),\n    su_attempted=r[14],\n    num_root=int(r[15]),\n    num_file_creations=int(r[16]),\n    label=r[-1]\n    )\n)\nparsed_rdd.take(5)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Spark analysis\n\nOne way to analyze data in Spark is to call methods on a dataframe."}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\n\nsqlContext = SQLContext(sc)\ndf = sqlContext.createDataFrame(parsed_rdd)\nconnections_by_protocol = df.groupBy('protocol_type').count().orderBy('count', ascending=False)\nconnections_by_protocol.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "Another way is to use Spark SQL"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\n\ndf.registerTempTable(\"connections\")\nattack_stats = sqlContext.sql(\"\"\"\n                           SELECT \n                             protocol_type, \n                             CASE label\n                               WHEN 'normal.' THEN 'no attack'\n                               ELSE 'attack'\n                             END AS state,\n                             COUNT(*) as total_freq,\n                             ROUND(AVG(src_bytes), 2) as mean_src_bytes,\n                             ROUND(AVG(dst_bytes), 2) as mean_dst_bytes,\n                             ROUND(AVG(duration), 2) as mean_duration,\n                             SUM(num_failed_logins) as total_failed_logins,\n                             SUM(num_compromised) as total_compromised,\n                             SUM(num_file_creations) as total_file_creations,\n                             SUM(su_attempted) as total_root_attempts,\n                             SUM(num_root) as total_root_acceses\n                           FROM connections\n                           GROUP BY protocol_type, state\n                           ORDER BY 3 DESC\n                           \"\"\")\nattack_stats.show()"}, {"cell_type": "code", "execution_count": 7, "metadata": {"scrolled": true}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\n\nax = attack_stats.toPandas().plot.bar(x='protocol_type', subplots=True, figsize=(10,25))"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\nax[0].get_figure().savefig('report.png');"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\nimport google.cloud.storage as gcs\nbucket = gcs.Client().get_bucket(BUCKET)\nfor blob in bucket.list_blobs(prefix='sparktodp/'):\n    blob.delete()\nbucket.blob('sparktodp/report.png').upload_from_filename('report.png')"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\nconnections_by_protocol.write.format(\"csv\").mode(\"overwrite\").save(\n    \"gs://{}/sparktodp/connections_by_protocol\".format(BUCKET))"}, {"cell_type": "markdown", "metadata": {}, "source": "## Test automation"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Writing to qwiklabs-gcp-01-85289ef223f9\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n22/06/02 15:01:34 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n22/06/02 15:01:34 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n22/06/02 15:01:34 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n22/06/02 15:01:34 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n22/06/02 15:01:44 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #0,5,main]) interrupted: \njava.lang.InterruptedException\n\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n\tat com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n\tat org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n\tat org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n22/06/02 15:01:44 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #1,5,main]) interrupted: \njava.lang.InterruptedException\n\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n\tat com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n\tat org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n\tat org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n+-------------+------+                                                          \n|protocol_type| count|\n+-------------+------+\n|         icmp|283602|\n|          tcp|190065|\n|          udp| 20354|\n+-------------+------+\n\n+-------------+---------+----------+--------------+--------------+-------------+-------------------+-----------------+--------------------+-------------------+------------------+\n|protocol_type|    state|total_freq|mean_src_bytes|mean_dst_bytes|mean_duration|total_failed_logins|total_compromised|total_file_creations|total_root_attempts|total_root_acceses|\n+-------------+---------+----------+--------------+--------------+-------------+-------------------+-----------------+--------------------+-------------------+------------------+\n|         icmp|   attack|    282314|        932.14|           0.0|          0.0|                  0|                0|                   0|                0.0|                 0|\n|          tcp|   attack|    113252|       9880.38|        881.41|        23.19|                 57|             2269|                  76|                1.0|               152|\n|          tcp|no attack|     76813|       1439.31|       4263.97|        11.08|                 18|             2776|                 459|               17.0|              5456|\n|          udp|no attack|     19177|         98.01|         89.89|      1054.63|                  0|                0|                   0|                0.0|                 0|\n|         icmp|no attack|      1288|         91.47|           0.0|          0.0|                  0|                0|                   0|                0.0|                 0|\n|          udp|   attack|      1177|          27.5|          0.23|          0.0|                  0|                0|                   0|                0.0|                 0|\n+-------------+---------+----------+--------------+--------------+-------------+-------------------+-----------------+--------------------+-------------------+------------------+\n\n                                                                                \r"}], "source": "BUCKET_list = !gcloud info --format='value(config.project)'\nBUCKET=BUCKET_list[0]\nprint('Writing to {}'.format(BUCKET))\n!/opt/conda/miniconda3/bin/python spark_analysis.py --bucket=$BUCKET"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "gs://qwiklabs-gcp-01-85289ef223f9/sparktodp/connections_by_protocol/\r\ngs://qwiklabs-gcp-01-85289ef223f9/sparktodp/connections_by_protocol/_SUCCESS\r\ngs://qwiklabs-gcp-01-85289ef223f9/sparktodp/connections_by_protocol/part-00000-0d96f7bf-203d-468b-9aa0-4179e6c9514a-c000.csv\r\ngs://qwiklabs-gcp-01-85289ef223f9/sparktodp/connections_by_protocol/part-00001-0d96f7bf-203d-468b-9aa0-4179e6c9514a-c000.csv\r\ngs://qwiklabs-gcp-01-85289ef223f9/sparktodp/connections_by_protocol/part-00002-0d96f7bf-203d-468b-9aa0-4179e6c9514a-c000.csv\r\ngs://qwiklabs-gcp-01-85289ef223f9/sparktodp/report.png\r\n"}], "source": "# List created elements :\n!gsutil ls gs://$BUCKET/sparktodp/**"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Copying file://spark_analysis.py [Content-Type=text/x-python]...\n/ [1 files][  2.8 KiB/  2.8 KiB]                                                \nOperation completed over 1 objects/2.8 KiB.                                      \n"}], "source": "# Save the created .py file to the bucket\n!gsutil cp spark_analysis.py gs://$BUCKET/sparktodp/spark_analysis.py"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.13"}}, "nbformat": 4, "nbformat_minor": 2}